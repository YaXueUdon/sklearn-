# -*- coding: utf-8 -*-
"""“report3_1_1.ipynb”

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WKE4fRqjl87GQcRTOPeyi_vu2R_mrMcl

# 1.1
"""

import numpy as np
import matplotlib.pylab as plt
import matplotlib.animation as animation
from matplotlib import colors
import seaborn as sns; sns.set()

import sklearn
from sklearn import datasets
from sklearn.model_selection import train_test_split

import pandas as pd

np.random.seed(12345)

iris = datasets.load_iris()
# データをデータフレーム化しておこう

Xdat = iris['data']

# 各クラスの名前リストを作っておこう
clsdat = [iris['target_names'][cid] for cid in iris['target']]

dfDat = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])
dfCls = pd.DataFrame(data=clsdat, columns=['Class'])

# データフレームを結合しておく
df = pd.concat([dfCls, dfDat], axis=1)

"""特徴量は 'petal length' と 'petal width'を使ってみよう．"""

# 3, 4 カラムは所望の特徴量
df2 = df[df['Class'] != 'setosa'].iloc[:, [0, 3, 4]]

sns.pairplot(df2, hue='Class', palette="husl")

X = df2.iloc[:, 1:].values
ydum = pd.get_dummies(df2.iloc[:, 0]).values
y = 2 * ydum[:, 0].astype(int) - 1

Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.4)

# データが所望の型になっているか確認
plt.plot(Xtrn[ytrn==1, 0], Xtrn[ytrn==1, 1], 'ro', alpha=0.6)
plt.plot(Xtrn[ytrn==-1, 0], Xtrn[ytrn==-1, 1], 'bo', alpha=0.6)
plt.plot(Xtst[ytst==1, 0], Xtst[ytst==1, 1], 'r*', alpha=0.8)
plt.plot(Xtst[ytst==-1, 0], Xtst[ytst==-1, 1], 'b*', alpha=0.8)

print('学習データの幅', Xtrn.max(axis=0) - Xtrn.min(axis=0))
print('学習データの中心', Xtrn.mean(axis=0))

Etrx = Xtrn.mean(axis=0)
Xtrain = (Xtrn - Etrx) 

print('学習データの幅（補正後）', Xtrain.max(axis=0) - Xtrain.min(axis=0))
print('学習データの中心（補正後）', Xtrain.mean(axis=0))

# テストデータの方も補正しておこう

Xtest = (Xtst - Etrx) 

print('評価データの幅（補正後）', Xtest.max(axis=0) - Xtest.min(axis=0))
print('評価データの中心（補正後）', Xtest.mean(axis=0))

ytrain = ytrn
ytest = ytst

plt.plot(Xtrain[ytrain==1, 0], Xtrain[ytrain==1, 1], 'ro', alpha=0.6)
plt.plot(Xtrain[ytrain==-1, 0], Xtrain[ytrain==-1, 1], 'bo', alpha=0.6)
plt.plot(Xtest[ytest==1, 0], Xtest[ytest==1, 1], 'r*', alpha=0.8)
plt.plot(Xtest[ytest==-1, 0], Xtest[ytest==-1, 1], 'b*', alpha=0.8)

"""# Fisher's LDA による判別

"""

def predictLDA(x, w, w0):
    return np.sign(w @ x.T + w0)


def SolverLDA(x, y):
    '''
    x はベクトル(D次元なので (N, D) な行列として受け取る)，y はラベル（N次元）
    LDA で回帰平面を求めるソルバー
    '''   
    # とりあえずデータの中心化から
    Ex = x.mean(axis=0)
    newx = x - Ex
   
    x1 = newx[y == 1, :]
    x2 = newx[y == -1, :]
    N1 = np.sum(y == 1)
    N2 = np.sum(y == -1)
    
    u1 = x1.mean(axis=0)
    u2 = x2.mean(axis=0)
    
    Sw = (x1 - u1).T @ (x1 - u1) / N1 + (x2 - u2).T @ (x2 - u2) / N2
    
    w = np.linalg.solve(Sw, u1 - u2)
    w0 = 0
    
    # あとは中心化で平行移動した分をずらしておく
    # y(x) = w^T newx + w0 として解いていて newx = x - Ex なので
    # w^T (x - Ex) + w0 = 0 を開くと w^T x - w^T Ex + w0 = 0
    
    w0 = w0 - w @ Ex
    
    return w, w0

# 解いてみよう

w, w0 = SolverLDA(Xtrain, ytrain)


# データ点の描画
plt.plot(Xtrain[ytrain==1, 0], Xtrain[ytrain==1, 1], 'ro', alpha=0.5, label='Versicolor(Train)')
plt.plot(Xtrain[ytrain!=1, 0], Xtrain[ytrain!=1, 1], 'bo', alpha=0.5, label='Virginica(Train)')
plt.plot(Xtest[ytest==1, 0], Xtest[ytest==1, 1], 'r*', alpha=0.8, label='Versilocor(Test)')
plt.plot(Xtest[ytest!=1, 0], Xtest[ytest!=1, 1], 'b*', alpha=0.8, label='Virgica(Test)')

# 解の直線は，w^T x + w0 = 0 なので， w1 x1 + w2 x2 + w0 = 0 なので
# x2 = -1/w2 (w1 x1 + w0)


xx1 = np.linspace(-2, 2)
xx2 = (w[0] * xx1 + w0) / (-w[1])
plt.plot(xx1, xx2, 'g-', linewidth=3)

plt.legend()
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.title('Fisher LDA Classification result')

"""それなりにまともっぽい．
では，間違い個数も出しておこう．
"""

ypred = predictLDA(Xtest, w, w0)
yerr = np.count_nonzero(ypred - ytest)
    
print('間違え個数: ', yerr)

"""ちなみに LDA は sklearn の中で実装済みで，
もっと簡単に求まる
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# あとはお作法に従って
clf = LDA()
clf.fit(Xtrain, ytrain)
ypred = clf.predict(Xtest)

# 実質２行くらい
yerr = np.count_nonzero(ypred - ytest)

print('誤り個数', yerr)

print('スクラッチ版の w, w0', w, w0)
print('sklearn の coef', clf.coef_, clf.intercept_)

"""## sklearn の ロジスティック回帰

"""

from sklearn.linear_model import LogisticRegression

# sklearn のロジスティック回帰は L2 正則化が入っていて，これの効きの大きさの逆数をパラメータCで与える
clf = LogisticRegression(C=1000)
clf.fit(Xtrain, ytrain)

w = np.array([clf.intercept_[0], clf.coef_[0][0], clf.coef_[0][1]])
ww = w / np.sqrt(w @ w)
print(ww)

ypred = clf.predict(Xtest)

yerr = np.count_nonzero(ypred - ytest)

print('誤り個数', yerr)

fig = plt.figure()
plt.xlim(-2, 2)
plt.ylim(-2, 2) 

# データ点の描画
plt.plot(Xtrain[ytrain==1, 0], Xtrain[ytrain==1, 1], 'ro', alpha=0.5, label='Versicolor(Train)')
plt.plot(Xtrain[ytrain!=1, 0], Xtrain[ytrain!=1, 1], 'bo', alpha=0.5, label='Virginica(Train)')
plt.plot(Xtest[ytest==1, 0], Xtest[ytest==1, 1], 'r*', alpha=0.8, label='Versilocor(Test)')
plt.plot(Xtest[ytest!=1, 0], Xtest[ytest!=1, 1], 'b*', alpha=0.8, label='Virgica(Test)')

# 解の直線は，w^T x + w0 = 0 なので， w1 x1 + w2 x2 + w0 = 0 なので
# x2 = -1/w2 (w1 x1 + w0)

xx1 = np.linspace(-2, 2)
xx2 = (ww[1] * xx1 + ww[0]) / (-ww[2])
plt.plot(xx1, xx2, 'g-', linewidth=3)

plt.legend()
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.title('sklearn IRLS  result')